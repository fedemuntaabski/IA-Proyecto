# Pictionary Live üé®

Aplicaci√≥n Python interactiva para jugar **Pictionary en vivo** usando detecci√≥n de gestos con las manos y clasificaci√≥n de sketches con inteligencia artificial.

## Caracter√≠sticas

- üé• **Captura en tiempo real**: Lee video de la c√°mara web
- ‚úã **Detecci√≥n de manos**: Usa MediaPipe para tracking de manos en 3D
- ‚úçÔ∏è **Acumulaci√≥n de trazo**: Detecta cuando dibujas en el aire y acumula la trayectoria
- ü§ñ **Inferencia en vivo**: Clasifica sketches usando modelos Keras/TensorFlow
- üìä **Predicci√≥n visualizada**: Muestra top-1 y top-3 predicciones en pantalla
- üíæ **Logging autom√°tico**: Registra cada inferencia con timestamp
- üñºÔ∏è **Captura de screenshots**: Guarda predicciones en `./predictions/`

## Requisitos

### Python
- Python 3.8+

### Dependencias principales
- `opencv-python` ‚Äî captura y procesamiento de video
- `tensorflow` (o `tensorflow-cpu` para CPU) ‚Äî cargar y ejecutar modelos Keras
- `mediapipe` ‚Äî detecci√≥n de manos (recomendado)
- `numpy` ‚Äî procesamiento de arrays
- `ndjson` ‚Äî lectura de archivos NDJSON (si se necesita explorar datos)

### Estructura de carpeta `IA`
Debe contener los siguientes archivos:
```
IA/
‚îú‚îÄ‚îÄ model_info.json                          # Metadatos del modelo
‚îú‚îÄ‚îÄ sketch_classifier_model.keras            # Modelo Keras (preferido)
‚îú‚îÄ‚îÄ sketch_classifier_model.h5               # Modelo alternativo (HDF5)
‚îú‚îÄ‚îÄ reduced_full_simplified_ambulance.ndjson # Datos de ejemplo
‚îî‚îÄ‚îÄ PictionaryTrainer.ipynb                  # Notebook de referencia (no se ejecuta)
```

#### Contenido de `model_info.json`
```json
{
  "input_shape": [28, 28, 1],           # (height, width, channels)
  "num_classes": 228,                    # N√∫mero de clases
  "classes": ["ambulance", "airplane", ...],  # Lista de etiquetas
  "test_accuracy": 0.8026,               # Accuracy del modelo
  "image_size": 28
}
```

#### Formato NDJSON
Cada l√≠nea es un JSON con un sketch:
```json
{
  "word": "ambulance",
  "drawing": [[[x1, x2, ...], [y1, y2, ...]], ...],  # Trazos (lista de lista de coordenadas)
  "recognized": true,
  "countrycode": "NL",
  ...
}
```

## Instalaci√≥n

### 1. Clonar o descargar el repositorio
```bash
cd tu_repo
```

### 2. Crear entorno virtual (recomendado)
```bash
python -m venv venv
# En Windows
venv\Scripts\activate
# En Linux/Mac
source venv/bin/activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

O instalar manualmente:
```bash
pip install opencv-python tensorflow mediapipe numpy ndjson
```

**Nota sobre TensorFlow:**
- Para GPU: `pip install tensorflow` (requiere CUDA/cuDNN)
- Para CPU: `pip install tensorflow-cpu`

## Uso

### Ejecuci√≥n b√°sica
```bash
python src/pictionary_live.py --ia-dir ./IA
```

### Con opciones
```bash
# Habilitar logging DEBUG
python src/pictionary_live.py --ia-dir ./IA --debug

# Usar c√°mara 1 en lugar de 0
python src/pictionary_live.py --ia-dir ./IA --camera-id 1

# Validar modelo sin abrir c√°mara (dry-run)
python src/pictionary_live.py --ia-dir ./IA --dry-run
```

### Controles en vivo
- **Dibujar**: Levanta la mano y mueve el dedo √≠ndice en el aire
- **Pausa detecci√≥n**: El trazo se clasifica autom√°ticamente cuando paras ~200ms
- **`s`** ‚Äî Guardar frame actual + predicci√≥n en `./predictions/`
- **`q`** ‚Äî Salir

## C√≥mo funciona

1. **Inicializaci√≥n**
   - Lee `model_info.json` (metadatos: tama√±o, etiquetas)
   - Carga modelo desde `*.keras` (preferido) o `*.h5`

2. **Captura y detecci√≥n**
   - Abre c√°mara web
   - Detecta landmarks de la mano con MediaPipe
   - Rastrea el dedo √≠ndice (landmark 8)

3. **Acumulaci√≥n de trazo**
   - Almacena puntos (x, y) normalizados mientras detecta movimiento
   - Cuando detecta pausa (200ms sin movimiento), dispara inferencia

4. **Preprocesado**
   - Normaliza puntos a canvas 28√ó28
   - Dibuja trazo (l√≠neas anti-aliased)
   - Normaliza valores a [0, 1]
   - Reshape a (28, 28, 1) para el modelo

5. **Inferencia**
   - Ejecuta `model.predict()`
   - Obtiene top-1 y top-3 predicciones
   - Muestra en overlay del video

6. **Logging**
   - Guarda cada predicci√≥n en `./inference.log`
   - Formato: `timestamp | etiqueta (prob%) | Top-3: ...`

## Salida

### Logs
- `logs/pictionary_YYYYMMDD_HHMMSS.log` ‚Äî Log completo de ejecuci√≥n (DEBUG/INFO)
- `inference.log` ‚Äî Log de inferencias (timestamp, etiqueta, probabilidad)

### Capturas
- `predictions/frame_YYYYMMDD_HHMMSS_ffffff.png` ‚Äî Frames guardados con `s`

## Arquitectura interna

```
PictionaryLive (aplicaci√≥n principal)
‚îú‚îÄ‚îÄ ModelLoader         ‚Üí Carga modelo y metadatos
‚îú‚îÄ‚îÄ HandTracker         ‚Üí Detecci√≥n de manos (MediaPipe)
‚îú‚îÄ‚îÄ StrokeAccumulator   ‚Üí Acumula puntos del trazo
‚îú‚îÄ‚îÄ DrawingPreprocessor ‚Üí Convierte trazo a imagen 28√ó28
‚îî‚îÄ‚îÄ [OpenCV UI]         ‚Üí Renderizado en pantalla
```

## Dependencias opcionales

### MediaPipe (recomendado)
Para mejor detecci√≥n de manos:
```bash
pip install mediapipe
```

Si no est√° disponible, el c√≥digo intenta usar detecci√≥n de contornos (fallback menos preciso).

## Troubleshooting

### "TensorFlow no est√° instalado"
```bash
pip install tensorflow
# O para CPU:
pip install tensorflow-cpu
```

### "C√°mara no se abre"
- Verifica que no est√© en uso por otra aplicaci√≥n
- Intenta con `--camera-id 1` o mayor
- En Linux, aseg√∫rate de tener permisos: `sudo usermod -a -G video $USER`

### "MediaPipe no disponible"
```bash
pip install mediapipe
```
Sin MediaPipe, el sistema usa fallback de movimiento (menos preciso).

### "model_info.json no encontrado"
- Verifica que la carpeta IA existe y est√° en la ruta correcta
- Usa: `python pictionary_live.py --ia-dir /ruta/a/IA`

### Bajo rendimiento en GPU
- Aseg√∫rate de que TensorFlow detecta GPU: `python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"`
- En Windows, verifica CUDA/cuDNN

## Ejemplos de uso

### Juego simple
```bash
cd e:\IA
python src\pictionary_live.py --ia-dir .\IA
```

### Debugging
```bash
python src\pictionary_live.py --ia-dir ./IA --debug
# Verifica logs en logs/pictionary_*.log
```

### Validar configuraci√≥n antes de jugar
```bash
python src\pictionary_live.py --ia-dir ./IA --dry-run
# Muestra: modelo cargado, clases disponibles, etc.
```

## Notas t√©cnicas

### Preprocesado
- Puntos de entrada: normalizados a [0, 1] desde landmarks de MediaPipe
- Canvas: 28√ó28 (blanco = 255, trazo = 0)
- Normalizaci√≥n: [0, 255] ‚Üí [0, 1]
- Shape final: (28, 28, 1) para modelo CNN

### Detecci√≥n de pausa
- Umbral: 200ms sin nuevos puntos
- M√≠nimo de puntos: 5 (para evitar ruido)

### Modelo
- Entrada: (28, 28, 1) ‚Äî escala de grises
- Salida: 228 clases (Quick, Draw! dataset)
- Accuracy: ~80.3%

## Licencia

Interno ‚Äî Proyecto IA

## Autor

Generado con Copilot

---

¬øPreguntas? Revisa los logs en `logs/` para m√°s detalles.
